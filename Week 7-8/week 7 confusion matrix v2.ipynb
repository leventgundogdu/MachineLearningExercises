{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is used in classification problems to assess where errors in the model were made. It is easy to see which predictions are wrong by using this table.\n",
    "\n",
    "Confusion matrixes can be created by predictions made from a logistic regression.\n",
    "\n",
    "Logistic regression aims to solve classification problems. It does this by predicting categorical outcomes, unlike linear regression that predicts a continuous outcome. In the simplest case there are two outcomes which is called binomial.\n",
    "\n",
    "### Example:\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "Given a sample of 12 individuals, 8 that have been diagnosed with cancer and 4 that are cancer-free, where individuals with cancer belong to class 1 (positive) and non-cancer individuals belong to class 0 (negative), we can display that data as follows:\n",
    "\n",
    "Individual Number\t    1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\n",
    "Actual Classification\t1\t1\t1\t1\t1\t1\t1\t1\t0\t 0\t 0\t 0\n",
    "\n",
    "Assume that we have a classifier that distinguishes between individuals with and without cancer in some way, we can take the 12 individuals and run them through the classifier. The classifier then makes 9 accurate predictions and misses 3: 2 individuals with cancer wrongly predicted as being cancer-free (sample 1 and 2), and 1 person without cancer that is wrongly predicted to have cancer (sample 9).\n",
    "\n",
    "Individual Number       \t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\n",
    "Actual Classification\t    1\t1\t1\t1\t1\t1\t1\t1\t0\t 0\t 0\t 0\n",
    "Predicted Classification\t0\t0\t1\t1\t1\t1\t1\t1\t1\t 0\t 0\t 0\n",
    "\n",
    "Notice, that if we compare the actual classification set to the predicted classification set, there are 4 different outcomes that could result in any particular column. One, if the actual classification is positive and the predicted classification is positive (1,1), this is called a true positive result because the positive sample was correctly identified by the classifier. Two, if the actual classification is positive and the predicted classification is negative (1,0), this is called a false negative result because the positive sample is incorrectly identified by the classifier as being negative. Third, if the actual classification is negative and the predicted classification is positive (0,1), this is called a false positive result because the negative sample is incorrectly identified by the classifier as being positive. Fourth, if the actual classification is negative and the predicted classification is negative (0,0), this is called a true negative result because the negative sample gets correctly identified by the classifier.\n",
    "\n",
    "We can then perform the comparison between actual and predicted classifications and add this information to the table.\n",
    "\n",
    "Individual Number\t        1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\n",
    "Actual Classification\t    1\t1\t1\t1\t1\t1\t1\t1\t0\t 0\t 0\t 0\n",
    "Predicted Classification\t0\t0\t1\t1\t1\t1\t1\t1\t1\t 0\t 0\t 0\n",
    "Result\t                   FN\tFN\tTP\tTP\tTP\tTP\tTP\tTP\tFP\tTN\tTN\tTN\n",
    "\n",
    "The template for any binary confusion matrix uses the four kinds of results discussed above (true positives, false negatives, false positives, and true negatives) along with the positive and negative classifications. The four outcomes can be formulated in a 2Ã—2 confusion matrix, as in https://en.wikipedia.org/wiki/Confusion_matrix:\n",
    "\n",
    "The color convention of the three data tables above were picked to match this confusion matrix, in order to easily differentiate the data.\n",
    "\n",
    "Now, we can simply total up each type of result, substitute into the template, and create a confusion matrix that will concisely summarize the results of testing the classifier.\n",
    "\n",
    "In this confusion matrix at https://en.wikipedia.org/wiki/Confusion_matrix, of the 8 samples with cancer, the system judged that 2 were cancer-free, and of the 4 samples without cancer, it predicted that 1 did have cancer. All correct predictions are located in the diagonal of the table (highlighted in green), so it is easy to visually inspect the table for prediction errors, as values outside the diagonal will represent them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual    = [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "predicted = [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three lines to make our compiler able to draw:\n",
    "#import sys\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg') # For more information about different kinds of backends see \n",
    "                        # https://matplotlib.org/stable/users/explain/backends.html\n",
    "\n",
    "# In order to create the confusion matrix we need to import metrics from the sklearn module.\n",
    "from sklearn import metrics\n",
    "\n",
    "# Once metrics is imported we can use the confusion matrix function on our actual and predicted values.\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "\n",
    "# To create a more interpretable visual display we need to convert the table into a confusion matrix display.\n",
    "cm_display = metrics.ConfusionMatrixDisplay (confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "# Vizualizing the display requires that we import pyplot from matplotlib.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Finally to display the plot we can use the functions plot() and show() from pyplot.\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "#Two  lines to make our compiler able to draw:\n",
    "#plt.savefig(sys.stdout.buffer)\n",
    "#sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created Metrics\n",
    "\n",
    "The matrix provides us with many useful metrics that help us to evaluate out classification model.\n",
    "\n",
    "The different measures include: Accuracy, Precision, Sensitivity (Recall), Specificity, and the F-score.\n",
    "\n",
    "Accuracy measures how often the model is correct.\n",
    "Accuracy = (True Positive + True Negative) / Total Predictions  -- 9/12 in our example\n",
    "\n",
    "Precision: Of the positives predicted, what percentage is truly positive? Precision does not evaluate the correctly predicted negative cases.\n",
    "Precision = True Positive / (True Positive + False Positive) -- 6/7 in our example\n",
    "\n",
    "Sensitivity (Recall)\n",
    "Of all the positive cases, what percentage are predicted positive?\n",
    "Sensitivity (sometimes called Recall) measures how good the model is at predicting positives.\n",
    "This means it looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).\n",
    "Sensitivity is good at understanding how well the model predicts something is positive.\n",
    "Sensitivity = True Positive / (True Positive + False Negative) -- 6/8 in our example\n",
    "\n",
    "Specificity\n",
    "How well the model is at prediciting negative results?\n",
    "Specificity is similar to sensitivity, but looks at it from the persepctive of negative results.\n",
    "Since it is just the opposite of Recall, we use the recall_score function, taking the opposite position label.\n",
    "Specificity = True Negative / (True Negative + False Positive) -- 3/4 in our example\n",
    "\n",
    "F-score\n",
    "F-score is the \"harmonic mean\" of precision and sensitivity.\n",
    "It considers both false positive and false negative cases and is good for imbalanced datasets.\n",
    "This score does not take into consideration the True Negative values.\n",
    "F-score = 2 * ((Precision * Sensitivity) / (Precision + Sensitivity)) -- 2 * ((0.85 * 0.75) / (0.85 + 0.75)) = 0.796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.75\n",
      "Precision is 0.8571428571428571\n",
      "Sensitivity recall is 0.75\n",
      "Specificity is 0.75\n",
      "F1 score is 0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "print (\"Accuracy is\", Accuracy)\n",
    "\n",
    "Precision = metrics.precision_score(actual, predicted)\n",
    "print (\"Precision is\", Precision)\n",
    "\n",
    "Sensitivity_recall = metrics.recall_score(actual, predicted)\n",
    "print (\"Sensitivity recall is\", Sensitivity_recall)\n",
    "\n",
    "Specificity = metrics.recall_score(actual, predicted, pos_label=0)\n",
    "print (\"Specificity is\", Specificity)\n",
    "\n",
    "F1_score = metrics.f1_score(actual, predicted)\n",
    "print (\"F1 score is\", F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
